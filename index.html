<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>LAHJATBERT: Multi-Label Arabic Dialect Identification | MBZUAI Research</title>
  <meta name="description" content="Curriculum learning and pseudo-labeling improve generalization of multi-label Arabic dialect identification models. Research from MBZUAI achieving 69% macro F1 on MLADI benchmark.">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=Space+Grotesk:wght@700&display=swap" rel="stylesheet">
  <style>
    :root{
      --text:#0a0a0a;
      --muted:#525252;
      --muted2:#737373;
      --link:#2563eb;
      --btn:#18181b;
      --btnText:#fafafa;
      --bg:#ffffff;
      --card-bg:#f9fafb;
      --border:#e5e7eb;
      --accent:#3b82f6;
      --success:#10b981;
    }

    *{ box-sizing:border-box; }
    html,body{ 
      margin:0; 
      padding:0; 
      background:var(--bg); 
      color:var(--text);
      -webkit-font-smoothing: antialiased;
      -moz-osx-font-smoothing: grayscale;
    }
    body{
      font-family: 'Inter', ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, sans-serif;
      line-height: 1.6;
    }

    .wrap{
      width:min(1150px, calc(100% - 48px));
      margin:0 auto;
      padding: 64px 0 80px;
    }

    .hero {
      text-align:center;
      margin-bottom: 48px;
    }

    h1{
      margin: 0 0 24px;
      font-family: 'Space Grotesk', 'Inter', ui-sans-serif, system-ui, sans-serif;
      font-weight: 700;
      letter-spacing: -0.03em;
      line-height: 1.15;
      font-size: clamp(32px, 4.5vw, 56px);
      color: #0a0a0a;
      text-wrap: balance;
    }

    .authors{
      margin: 0 auto 12px;
      max-width: 980px;
      font-size: clamp(15px, 1.5vw, 22px);
      line-height: 1.6;
      font-weight: 500;
      letter-spacing: -0.01em;
    }

    .authors .a{
      color: var(--link);
      text-decoration: none;
      white-space: nowrap;
      pointer-events:none;
      cursor:default;
      font-weight: 500;
    }

    sup{
      font-size: 0.7em;
      vertical-align: super;
      font-weight: 600;
      margin-left: 1px;
    }

    .affil{
      margin: 16px 0 8px;
      font-size: clamp(15px, 1.35vw, 20px);
      color: var(--muted);
      font-weight: 500;
      letter-spacing: -0.01em;
    }

    .notes{
      margin: 8px 0 10px;
      font-size: clamp(13px, 1.15vw, 17px);
      color: var(--muted2);
      font-weight: 500;
      font-style: italic;
    }

    .email{
      margin: 8px 0 28px;
      font-size: clamp(13px, 1.15vw, 17px);
      color: var(--muted2);
      font-weight: 500;
      font-family: 'SF Mono', 'Monaco', 'Consolas', monospace;
      letter-spacing: -0.02em;
    }

    .btnrow{
      display:flex;
      justify-content:center;
      flex-wrap:wrap;
      gap: 12px;
      margin: 24px 0 48px;
    }

    .btn{
      display:inline-flex;
      align-items:center;
      gap: 9px;
      padding: 13px 22px;
      border: 0;
      border-radius: 10px;
      background: var(--btn);
      color: var(--btnText);
      font-weight: 600;
      font-size: 15px;
      line-height: 1;
      letter-spacing: -0.01em;
      cursor: pointer;
      user-select:none;
      text-decoration: none;
      -webkit-tap-highlight-color: transparent;
      transition: transform 150ms cubic-bezier(0.4, 0, 0.2, 1), 
                  box-shadow 150ms cubic-bezier(0.4, 0, 0.2, 1);
      box-shadow: 0 1px 3px rgba(0, 0, 0, 0.12), 0 1px 2px rgba(0, 0, 0, 0.08);
    }
    
    .btn svg{ 
      width: 18px; 
      height: 18px; 
      fill: currentColor; 
      opacity: 0.95;
      flex-shrink: 0;
    }

    .btn:hover{ 
      transform: translateY(-2px); 
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15), 0 2px 4px rgba(0, 0, 0, 0.1);
    }
    
    .btn:active{ 
      transform: translateY(0px); 
      box-shadow: 0 1px 3px rgba(0, 0, 0, 0.12);
    }
    
    .btn:focus-visible{ 
      outline: 3px solid rgba(37, 99, 235, 0.4); 
      outline-offset: 2px; 
    }

    .btn-disabled {
      background: #6b7280;
      cursor: not-allowed;
      opacity: 0.7;
      position: relative;
    }

    .btn-disabled:hover {
      transform: none;
      box-shadow: 0 1px 3px rgba(0, 0, 0, 0.12), 0 1px 2px rgba(0, 0, 0, 0.08);
    }

    .btn-disabled::after {
      content: 'Coming Soon';
      position: absolute;
      bottom: calc(100% + 8px);
      left: 50%;
      transform: translateX(-50%) scale(0.9);
      background: #1f2937;
      color: white;
      padding: 6px 12px;
      border-radius: 6px;
      font-size: 13px;
      white-space: nowrap;
      opacity: 0;
      pointer-events: none;
      transition: opacity 200ms ease, transform 200ms ease;
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }

    .btn-disabled::before {
      content: '';
      position: absolute;
      bottom: calc(100% + 2px);
      left: 50%;
      transform: translateX(-50%);
      border: 6px solid transparent;
      border-top-color: #1f2937;
      opacity: 0;
      pointer-events: none;
      transition: opacity 200ms ease;
    }

    .btn-disabled:hover::after,
    .btn-disabled:hover::before {
      opacity: 1;
      transform: translateX(-50%) scale(1);
    }

    /* Content Sections */
    .section {
      margin-bottom: 56px;
    }

    .section-title {
      font-size: clamp(24px, 3vw, 32px);
      font-weight: 700;
      margin: 0 0 24px;
      letter-spacing: -0.02em;
      font-family: 'Space Grotesk', 'Inter', sans-serif;
    }

    .abstract {
      background: var(--card-bg);
      border: 1px solid var(--border);
      border-radius: 12px;
      padding: 32px;
      font-size: clamp(15px, 1.4vw, 18px);
      line-height: 1.7;
      color: var(--text);
      margin-bottom: 48px;
    }

    .abstract p {
      margin: 0 0 16px;
    }

    .abstract p:last-child {
      margin-bottom: 0;
    }

    .highlights {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
      gap: 24px;
      margin-bottom: 48px;
    }

    .highlight-card {
      background: var(--card-bg);
      border: 1px solid var(--border);
      border-radius: 12px;
      padding: 24px;
      transition: all 200ms ease;
    }

    .highlight-card:hover {
      border-color: var(--accent);
      box-shadow: 0 4px 12px rgba(59, 130, 246, 0.1);
      transform: translateY(-2px);
    }

    .highlight-icon {
      width: 48px;
      height: 48px;
      background: linear-gradient(135deg, var(--accent) 0%, #2563eb 100%);
      border-radius: 10px;
      display: flex;
      align-items: center;
      justify-content: center;
      margin-bottom: 16px;
      color: white;
      font-weight: 700;
      font-size: 24px;
    }

    .highlight-title {
      font-size: 18px;
      font-weight: 700;
      margin: 0 0 8px;
      letter-spacing: -0.01em;
    }

    .highlight-text {
      font-size: 15px;
      color: var(--muted);
      line-height: 1.6;
      margin: 0;
    }

    .metrics {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: 20px;
      margin-bottom: 48px;
    }

    .metric-card {
      background: linear-gradient(135deg, #f0f9ff 0%, #e0f2fe 100%);
      border: 1px solid #bae6fd;
      border-radius: 12px;
      padding: 24px;
      text-align: center;
    }

    .metric-value {
      font-size: clamp(32px, 4vw, 48px);
      font-weight: 800;
      color: var(--accent);
      font-family: 'Space Grotesk', sans-serif;
      line-height: 1;
      margin-bottom: 8px;
    }

    .metric-label {
      font-size: 14px;
      font-weight: 600;
      color: var(--muted);
      text-transform: uppercase;
      letter-spacing: 0.05em;
    }

    .contributions {
      background: var(--card-bg);
      border-left: 4px solid var(--accent);
      border-radius: 8px;
      padding: 28px 32px;
      margin-bottom: 48px;
    }

    .contributions-title {
      font-size: 20px;
      font-weight: 700;
      margin: 0 0 20px;
      letter-spacing: -0.01em;
    }

    .contributions ul {
      margin: 0;
      padding-left: 24px;
      list-style: none;
    }

    .contributions li {
      position: relative;
      font-size: 16px;
      line-height: 1.7;
      margin-bottom: 12px;
      padding-left: 8px;
    }

    .contributions li::before {
      content: "‚Üí";
      position: absolute;
      left: -24px;
      color: var(--accent);
      font-weight: 700;
    }

    .contributions li:last-child {
      margin-bottom: 0;
    }

    .tech-details {
      background: #fefce8;
      border: 1px solid #fde047;
      border-radius: 12px;
      padding: 28px 32px;
      margin-bottom: 48px;
    }

    .tech-title {
      font-size: 20px;
      font-weight: 700;
      margin: 0 0 16px;
      letter-spacing: -0.01em;
      display: flex;
      align-items: center;
      gap: 10px;
    }

    .tech-icon {
      width: 28px;
      height: 28px;
      background: #facc15;
      border-radius: 6px;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 16px;
    }

    .tech-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
      gap: 20px;
    }

    .tech-item {
      background: white;
      border-radius: 8px;
      padding: 16px;
    }

    .tech-item-title {
      font-weight: 700;
      font-size: 15px;
      margin-bottom: 6px;
      color: var(--text);
    }

    .tech-item-text {
      font-size: 14px;
      color: var(--muted);
      margin: 0;
    }

    .citation {
      background: #f3f4f6;
      border: 1px solid var(--border);
      border-radius: 12px;
      padding: 24px;
      font-family: 'SF Mono', 'Monaco', 'Consolas', monospace;
      font-size: 13px;
      line-height: 1.6;
      overflow-x: auto;
      margin-bottom: 48px;
    }

    .citation-title {
      font-family: 'Inter', sans-serif;
      font-size: 18px;
      font-weight: 700;
      margin: 0 0 16px;
    }

    .citation pre {
      margin: 0;
      white-space: pre-wrap;
      word-wrap: break-word;
    }

    @media (max-width: 700px){
      .wrap{ padding: 48px 0 64px; }
      .authors .a{ white-space: normal; }
      .btn{ font-size: 14px; padding: 12px 20px; gap: 8px; }
      h1{ margin-bottom: 28px; }
      .abstract{ padding: 24px; }
      .highlights{ grid-template-columns: 1fr; }
      .metrics{ grid-template-columns: 1fr; }
      .tech-grid{ grid-template-columns: 1fr; }
    }
  </style>
</head>

<body>
  <div class="wrap">
    <!-- Hero Section -->
    <div class="hero">
      <h1>
        Curriculum Learning and Pseudo-Labeling Improve the Generalization of <br/>
        Multi-Label Arabic Dialect Identification Models
      </h1>

      <div class="authors" aria-label="Authors">
        <span class="a">Ali Mekky</span><sup>1*</sup>,&nbsp;
        <span class="a">Mohamed El Zeftawy</span><sup>1*</sup>,&nbsp;
        <span class="a">Lara Hassan</span><sup>1*</sup><br/>
        <span class="a">Amr Keleg</span><sup>1</sup>,&nbsp;
        <span class="a">Preslav Nakov</span><sup>1</sup>
      </div>

      <div class="affil">
        <span><sup>1</sup>Mohamed Bin Zayed University of Artificial Intelligence</span>
      </div>

      <div class="notes">
        <span>* Equal contribution.</span>
      </div>

      <div class="email">
        {ali.mekky, mohamed.elzeftawy, lara.hassan, amr.keleg, preslav.nakov}@mbzuai.ac.ae
      </div>

      <div class="btnrow" aria-label="Project resources">
        <a class="btn" href="https://arxiv.org/abs/2602.12937" target="_blank" rel="noopener noreferrer" aria-label="Paper">
          <svg viewBox="0 0 24 24" aria-hidden="true">
            <path d="M6 2h8l4 4v16a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2zm8 1.5V7h3.5L14 3.5zM7 11h10v2H7v-2zm0 4h10v2H7v-2zm0-8h6v2H7V7z"/>
          </svg>
          Paper
        </a>

        <a class="btn" href="https://huggingface.co/spaces/AHAAM/LahjatBERT" target="_blank" rel="noopener noreferrer" aria-label="Demo" style="background: linear-gradient(135deg, #ff7e5f 0%, #feb47b 100%);">
          <svg viewBox="0 0 24 24" aria-hidden="true">
            <path d="M14.94 5.19A4.38 4.38 0 0 0 16 2a4.44 4.44 0 0 0-3.67 1.85A4.44 4.44 0 0 0 9 2a4.38 4.38 0 0 0 1.06 3.19A5.26 5.26 0 0 0 7 10v1a4 4 0 0 0 4 4h2a4 4 0 0 0 4-4v-1a5.26 5.26 0 0 0-3.06-4.81zM12 18a6 6 0 0 1-6-6v-1h12v1a6 6 0 0 1-6 6zm-3 2h6v2H9v-2z"/>
          </svg>
          üöÄ Demo
        </a>

        <a class="btn btn-disabled" aria-label="Code - Coming Soon">
          <svg viewBox="0 0 24 24" aria-hidden="true">
            <path d="M12 .5C5.73.5.75 5.48.75 11.75c0 4.86 3.16 8.98 7.55 10.43.55.1.75-.24.75-.53v-1.9c-3.07.67-3.72-1.3-3.72-1.3-.5-1.28-1.22-1.62-1.22-1.62-1-.68.08-.67.08-.67 1.1.08 1.68 1.13 1.68 1.13.98 1.68 2.58 1.2 3.21.92.1-.71.38-1.2.69-1.48-2.45-.28-5.03-1.23-5.03-5.49 0-1.21.43-2.2 1.13-2.98-.11-.28-.49-1.4.11-2.93 0 0 .92-.3 3.01 1.14.87-.24 1.8-.36 2.73-.36.93 0 1.86.12 2.73.36 2.09-1.44 3.01-1.14 3.01-1.14.6 1.53.22 2.65.11 2.93.7.78 1.13 1.77 1.13 2.98 0 4.27-2.59 5.2-5.05 5.47.39.33.74 1 .74 2.02v2.99c0 .29.2.64.76.53 4.38-1.46 7.53-5.57 7.53-10.42C23.25 5.48 18.27.5 12 .5z"/>
          </svg>
          Code
        </a>

        <a class="btn btn-disabled" aria-label="Data - Coming Soon">
          <svg viewBox="0 0 24 24" aria-hidden="true">
            <path d="M12 2C7.03 2 3 3.79 3 6v12c0 2.21 4.03 4 9 4s9-1.79 9-4V6c0-2.21-4.03-4-9-4zm0 2c4.42 0 7 .98 7 2s-2.58 2-7 2-7-.98-7-2 2.58-2 7-2zm0 6c4.42 0 7-.98 7-2v3c0 1.02-2.58 2-7 2s-7-.98-7-2V8c0 1.02 2.58 2 7 2zm0 6c4.42 0 7-.98 7-2v3c0 1.02-2.58 2-7 2s-7-.98-7-2v-3c0 1.02 2.58 2 7 2z"/>
          </svg>
          Data
        </a>
      </div>
    </div>

    <!-- Performance Metrics -->
    <div class="section">
      <div class="metrics">
        <div class="metric-card">
          <div class="metric-value">69.04%</div>
          <div class="metric-label">Macro F1 Score</div>
        </div>
        <div class="metric-card">
          <div class="metric-value">+14%</div>
          <div class="metric-label">Over strongest previously reported system</div>
        </div>
        <div class="metric-card">
          <div class="metric-value">18</div>
          <div class="metric-label">Arabic Dialects</div>
        </div>
        <div class="metric-card">
          <div class="metric-value">Top #1</div>
          <div class="metric-label">MLADI Leaderboard</div>
        </div>
      </div>
    </div>

    <!-- Abstract -->
    <div class="section">
      <h2 class="section-title">Abstract</h2>
      <div class="abstract">
        <p>
          Arabic Dialect Identification (ADI) has traditionally been modeled as a single-label classification task. However, recent work argues that ADI should be framed as a multi-label classification problem, as a single utterance may simultaneously sound natural to speakers from multiple countries. Despite this recognition, ADI remains constrained by the availability of single-label datasets, with no large-scale multi-label resources available for training.
        </p>
        <p>
          By analyzing models trained on single-label ADI data, we demonstrate that the main difficulty in repurposing such datasets for Multi-Label Arabic Dialect Identification (MLADI) lies in the selection of negative samples, many sentences treated as negative could be acceptable in multiple dialects. To address these issues, we construct a multi-label dataset by generating automatic multi-label annotations using GPT-4o and binary dialect acceptability classifiers, with aggregation guided by the Arabic Level of Dialectness (ALDi).
        </p>
        <p>
          We then train a BERT-based multi-label classifier using curriculum learning strategies aligned with dialectal complexity and label cardinality. On the MLADI leaderboard, our best-performing LAHJATBERT model achieves a macro F1 of 0.69, compared to 0.55 for the strongest previously reported system, representing a significant advancement in multi-label Arabic dialect identification.
        </p>
      </div>
    </div>

    <!-- Key Highlights -->
    <div class="section">
      <h2 class="section-title">Key Highlights</h2>
      <div class="highlights">
        <div class="highlight-card">
          <div class="highlight-icon">üéØ</div>
          <h3 class="highlight-title">Multi-Label Framework</h3>
          <p class="highlight-text">
            Comprehensive approach to MLADI that explicitly models dialectal overlap, recognizing that sentences can be acceptable in multiple Arabic dialects simultaneously.
          </p>
        </div>

        <div class="highlight-card">
          <div class="highlight-icon">ü§ñ</div>
          <h3 class="highlight-title">Hybrid Pseudo-Labeling</h3>
          <p class="highlight-text">
            Novel dataset construction combining GPT-4o predictions with binary classifiers, guided by ALDi scores to balance precision and recall across dialectal complexity ranges.
          </p>
        </div>

        <div class="highlight-card">
          <div class="highlight-icon">üìö</div>
          <h3 class="highlight-title">Curriculum Learning</h3>
          <p class="highlight-text">
            Two curriculum strategies based on ALDi scores and label cardinality, progressively exposing the model to increasingly ambiguous dialectal instances.
          </p>
        </div>

        <div class="highlight-card">
          <div class="highlight-icon">üìä</div>
          <h3 class="highlight-title">State-of-the-Art Results</h3>
          <p class="highlight-text">
            Achieves 69.04% macro F1 on MLADI benchmark, surpassing previous best system by 14 percentage points and outperforming larger multilingual models.
          </p>
        </div>

        <div class="highlight-card">
          <div class="highlight-icon">üîç</div>
          <h3 class="highlight-title">Training Dynamics Analysis</h3>
          <p class="highlight-text">
            Deep analysis reveals that negative samples in single-label datasets often represent valid multi-dialectal cases, informing better pseudo-labeling strategies.
          </p>
        </div>

        <div class="highlight-card">
          <div class="highlight-icon">üåç</div>
          <h3 class="highlight-title">Broad Coverage</h3>
          <p class="highlight-text">
            Covers 18 country-level Arabic dialects including Algeria, Egypt, Iraq, Jordan, Lebanon, Morocco, Saudi Arabia, Syria, and others across the Arab world.
          </p>
        </div>
      </div>
    </div>

    <!-- Key Contributions -->
    <div class="section">
      <div class="contributions">
        <h3 class="contributions-title">Key Contributions</h3>
        <ul>
          <li>
            <strong>In-depth Analysis:</strong> Comprehensive examination of limitations in reusing single-label ADI datasets for multi-label dialect acceptability, demonstrating systematic issues with negative sample selection.
          </li>
          <li>
            <strong>Pseudo-Labeled Dataset:</strong> Construction of a multi-label training dataset by aggregating predictions from GPT-4o and 18 binary dialect classifiers, guided by ALDi scores for optimal precision-recall balance.
          </li>
          <li>
            <strong>LAHJATBERT Model Family:</strong> Introduction of BERT-based multi-label models trained with curriculum learning strategies, achieving state-of-the-art performance on the MLADI benchmark.
          </li>
        </ul>
      </div>
    </div>

    <!-- Technical Details -->
    <div class="section">
      <div class="tech-details">
        <h3 class="tech-title">
          <span class="tech-icon">‚öôÔ∏è</span>
          Technical Approach
        </h3>
        <div class="tech-grid">
          <div class="tech-item">
            <div class="tech-item-title">Base Model</div>
            <p class="tech-item-text">MARBERT (Abdul-Mageed et al., 2021) fine-tuned for multi-label classification with frozen bottom 8 layers</p>
          </div>
          <div class="tech-item">
            <div class="tech-item-title">Training Strategy</div>
            <p class="tech-item-text">Binary cross-entropy loss with curriculum learning, 3 epochs, batch size 24, sigmoid threshold 0.3</p>
          </div>
          <div class="tech-item">
            <div class="tech-item-title">Pseudo-Labeling</div>
            <p class="tech-item-text">GPT-4o for intermediate ALDi ranges (0.11-0.77), binary classifiers for extremes (&lt;0.11 or &gt;0.77)</p>
          </div>
          <div class="tech-item">
            <div class="tech-item-title">ALDi Integration</div>
            <p class="tech-item-text">Arabic Level of Dialectness scores guide both data construction and curriculum ordering for optimal learning</p>
          </div>
          <div class="tech-item">
            <div class="tech-item-title">Curriculum Types</div>
            <p class="tech-item-text">Two strategies: ALDi-based (dialectal complexity) and cardinality-based (number of valid dialect labels)</p>
          </div>
          <div class="tech-item">
            <div class="tech-item-title">Dataset Size</div>
            <p class="tech-item-text">Combined NADI 2020, 2021, 2023 datasets covering ~50,000 tweets across 18 Arabic dialects</p>
          </div>
        </div>
      </div>
    </div>

    <!-- Results Summary -->
    <div class="section">
      <h2 class="section-title">Performance Comparison</h2>
      <div class="abstract">
        <p><strong>MLADI Test Set Results (1,000 sentences, 11 dialects):</strong></p>
        <p style="margin-left: 20px;">
          ‚Ä¢ <strong>LAHJATBERT + ALDi CL:</strong> 69.0% macro F1 (65.0% precision, 76.4% recall)<br/>
          ‚Ä¢ <strong>LAHJATBERT + Cardinality CL:</strong> 66.6% macro F1 (59.3% precision, 81.0% recall)<br/>
          ‚Ä¢ <strong>LAHJATBERT (no curriculum):</strong> 68.0% macro F1 (69.0% precision, 69.7% recall)<br/>
          ‚Ä¢ <strong>Aya-32B:</strong> 54.5% macro F1<br/>
          ‚Ä¢ <strong>Elyadata:</strong> 52.4% macro F1<br/>
          ‚Ä¢ <strong>NADI 2024 Baseline:</strong> 47.0% macro F1
        </p>
        <p>
          The ALDi-based curriculum achieves the best overall balance, while the cardinality-based curriculum maximizes recall. All LAHJATBERT variants significantly outperform previous approaches, demonstrating the effectiveness of our pseudo-labeling and curriculum learning strategies.
        </p>
      </div>
    </div>

    <!-- Citation -->
    <div class="section">
      <div class="citation">
        <div class="citation-title">Citation</div>
        <pre>@misc{mekky2026curriculumlearningpseudolabelingimprove,
      title={Curriculum Learning and Pseudo-Labeling Improve the Generalization of Multi-Label Arabic Dialect Identification Models}, 
      author={Ali Mekky and Mohamed El Zeftawy and Lara Hassan and Amr Keleg and Preslav Nakov},
      year={2026},
      eprint={2602.12937},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2602.12937}, 
}</pre>
      </div>
    </div>

  </div>
</body>
</html>